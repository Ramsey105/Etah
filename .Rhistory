return(mf)
else if (method != "qr")
warning(gettextf("method = '%s' is not supported. Using 'qr'",
method), domain = NA)
mt <- attr(mf, "terms")
y <- model.response(mf, "numeric")
w <- as.vector(model.weights(mf))
if (!is.null(w) && !is.numeric(w))
stop("'weights' must be a numeric vector")
offset <- model.offset(mf)
mlm <- is.matrix(y)
ny <- if (mlm)
nrow(y)
else length(y)
if (!is.null(offset)) {
if (!mlm)
offset <- as.vector(offset)
if (NROW(offset) != ny)
stop(gettextf("number of offsets is %d, should equal %d (number of observations)",
NROW(offset), ny), domain = NA)
}
if (is.empty.model(mt)) {
x <- NULL
z <- list(coefficients = if (mlm) matrix(NA_real_, 0,
ncol(y)) else numeric(), residuals = y, fitted.values = 0 *
y, weights = w, rank = 0L, df.residual = if (!is.null(w)) sum(w !=
0) else ny)
if (!is.null(offset)) {
z$fitted.values <- offset
z$residuals <- y - offset
}
}
else {
x <- model.matrix(mt, mf, contrasts)
z <- if (is.null(w))
lm.fit(x, y, offset = offset, singular.ok = singular.ok,
...)
else lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok,
...)
}
class(z) <- c(if (mlm) "mlm", "lm")
z$na.action <- attr(mf, "na.action")
z$offset <- offset
z$contrasts <- attr(x, "contrasts")
z$xlevels <- .getXlevels(mt, mf)
z$call <- cl
z$terms <- mt
if (model)
z$model <- mf
if (ret.x)
z$x <- x
if (ret.y)
z$y <- y
if (!qr)
z$qr <- NULL
z
}
function (formula, data, subset, weights, na.action, method = "qr",
model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE,
contrasts = NULL, offset, ...)
{
ret.x <- x
ret.y <- y
cl <- match.call()
mf <- match.call(expand.dots = FALSE)
m <- match(c("formula", "data", "subset", "weights", "na.action",
"offset"), names(mf), 0L)
mf <- mf[c(1L, m)]
mf$drop.unused.levels <- TRUE
mf[[1L]] <- quote(stats::model.frame)
mf <- eval(mf, parent.frame())
if (method == "model.frame")
return(mf)
else if (method != "qr")
warning(gettextf("method = '%s' is not supported. Using 'qr'",
method), domain = NA)
mt <- attr(mf, "terms")
y <- model.response(mf, "numeric")
w <- as.vector(model.weights(mf))
if (!is.null(w) && !is.numeric(w))
stop("'weights' must be a numeric vector")
offset <- model.offset(mf)
mlm <- is.matrix(y)
ny <- if (mlm)
nrow(y)
else length(y)
if (!is.null(offset)) {
if (!mlm)
offset <- as.vector(offset)
if (NROW(offset) != ny)
stop(gettextf("number of offsets is %d, should equal %d (number of observations)",
NROW(offset), ny), domain = NA)
}
if (is.empty.model(mt)) {
x <- NULL
z <- list(coefficients = if (mlm) matrix(NA_real_, 0,
ncol(y)) else numeric(), residuals = y, fitted.values = 0 *
y, weights = w, rank = 0L, df.residual = if (!is.null(w)) sum(w !=
0) else ny)
if (!is.null(offset)) {
z$fitted.values <- offset
z$residuals <- y - offset
}
}
else {
x <- model.matrix(mt, mf, contrasts)
z <- if (is.null(w))
lm.fit(x, y, offset = offset, singular.ok = singular.ok,
...)
else lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok,
...)
}
class(z) <- c(if (mlm) "mlm", "lm")
z$na.action <- attr(mf, "na.action")
z$offset <- offset
z$contrasts <- attr(x, "contrasts")
z$xlevels <- .getXlevels(mt, mf)
z$call <- cl
z$terms <- mt
if (model)
z$model <- mf
if (ret.x)
z$x <- x
if (ret.y)
z$y <- y
if (!qr)
z$qr <- NULL
z
}
sorting <- function (.data, ..., .by_group = FALSE)
{
UseMethod("sorting")
}
?gradalgo
# Load necessary libraries
library(MASS)
library(dplyr)
library(ggplot2)
library(caret)
# Explore data
str(Boston)
summary(Boston)
# Split data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(Boston$medv, p = 0.7, list = FALSE)
trainData <- Boston[trainIndex,]
testData <- Boston[-trainIndex,]
# Fit multiple linear regression model
model <- lm(medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, data = trainData)
# Summarize model
summary(model)
# Make predictions on test data
predictions <- predict(model, testData)
# Evaluate model performance
rmse <- sqrt(mean((testData$medv - predictions)^2))
print(paste("Root Mean Squared Error (RMSE):", rmse))
# Visualize actual vs. predicted values
ggplot(data.frame(actual = testData$medv, predicted = predictions), aes(x = actual, y = predicted)) +
geom_point() +
geom_abline(intercept = 0, slope = 1, color = "red") +
labs(title = "Actual vs. Predicted House Prices", x = "Actual Price", y = "Predicted Price")
# Load necessary libraries
library(MASS)
library(ggplot2)
library(dplyr)
library(caret)
# Load Boston Housing data
data(Boston)
Boston
splitData <- function(data, indices) {
# Split data into training and testing sets
trainData <- data[indices, ]
testData <- data[-indices, ]
# Return list containing training and testing data
return(list(trainData = trainData, testData = testData))
#' Splitting of data
#'
#' @param data a number
#' @param indices a number
#'
#' @return a number
#' @export
#'
#' @examples
splitData <- function(data, indices) {
# Split data into training and testing sets
trainData <- data[indices, ]
testData <- data[-indices, ]
# Return list containing training and testing data
return(list(trainData = trainData, testData = testData))
}
yyyy
y
hf
v
i 8
8
#' Splitting of data
#'
#' @param data a number
#' @param indices a number
#'
#' @return a number
#' @export
#'
#' @examples
splitData <- function(data, indices) {
# Split data into training and testing sets
trainData <- data[indices, ]
testData <- data[-indices, ]
# Return list containing training and testing data
return(list(trainData = trainData, testData = testData))
}
libra*
kh
library(Gradalgo)
library(Gradalgo)
createDataPartition()
load("C:/Users/LENOVO/Desktop/FILES/My func/MASS/data/Boston.rda")
# Load necessary libraries
library(Gradalgo)
# Load Boston Housing data
data(Boston)
# Explore data
str(Boston)
summary(Boston)
# Split data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(Boston$medv, p = 0.7, list = FALSE)
trainData <- Boston[trainIndex,]
testData <- Boston[-trainIndex,]
# Fit multiple linear regression model
model <- lm(medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, data = trainData)
# Summarize model
summary(model)
# Make predictions on test data
predictions <- predict(model, testData)
# Evaluate model performance
rmse <- sqrt(mean((testData$medv - predictions)^2))
print(paste("Root Mean Squared Error (RMSE):", rmse))
# Visualize actual vs. predicted values
ggplot(data.frame(actual = testData$medv, predicted = predictions), aes(x = actual, y = predicted)) +
geom_point() +
geom_abline(intercept = 0, slope = 1, color = "red") +
labs(title = "Actual vs. Predicted House Prices", x = "Actual Price", y = "Predicted Price")
# Load necessary libraries
library(Gradalgo)
# Load Boston Housing data
data(Boston)
# Explore data
str(Boston)
summary(Boston)
# Split data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(Boston$medv, p = 0.7, list = FALSE)
trainData <- Boston[trainIndex,]
testData <- Boston[-trainIndex,]
# Fit multiple linear regression model
model <- lm(medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, data = trainData)
# Summarize model
summary(model)
# Make predictions on test data
predictions <- predict(model, testData)
# Evaluate model performance
rmse <- sqrt(mean((testData$medv - predictions)^2))
print(paste("Root Mean Squared Error (RMSE):", rmse))
# Visualize actual vs. predicted values
library(ggplot2)
ggplot(data.frame(actual = testData$medv, predicted = predictions), aes(x = actual, y = predicted)) +
geom_point() +
geom_abline(intercept = 0, slope = 1, color = "red") +
labs(title = "Actual vs. Predicted House Prices", x = "Actual Price", y = "Predicted Price")
labs
?createDataPartition
createDataPartition(y, p, list = FALSE)
?boston
view(Boston)
Boston
str(Boston)
set.seed(123)
trainIndex <- createDataPartition(Boston$medv, p = 0.7, list = FALSE)#creates partition of the data based on the medv column
trainIndex
max(trainIndex)
min(trainIndex)
trainIndex <- createDataPartition(Boston$medv, p = 0.7, list = FALSE)#creates partition of the data based on the medv column
#medv specifies the column to be used for partitioning
#p=0.7 proportion of the data to include the training set (70%)
trainData <- Boston[trainIndex,]
testData <- Boston[-trainIndex,]
data_split <- splitData(Boston, trainIndex)
trainData <-  data_split$trainData
testData <- data_split$testData
Gradalgo::splitData(Boston, trainIndex)
data_split <- splitData(Boston, trainIndex)
data_split
# Load necessary libraries
library(Gradalgo)
# Load Boston Housing data
data(Boston)
# Explore data
str(Boston)#displays the compact summary of the internal structure of the boston dataset
summary(Boston)#provides more detailed summary of each var in the boston dataset
# Split data into training and testing sets: This part splits the data set into 2 parts
#-a training set(70% of the data), - a testing set (30% of the data)
set.seed(123)# this pÃ ermites that the slpit is done thesame way every time the code is run
trainIndex <- createDataPartition(Boston$medv, p = 0.7, list = FALSE)#creates partition of the data based on the medv column
#medv specifies the column to be used for partitioning
#p=0.7 proportion of the data to include the training set (70%)
data_split <- splitData(Boston, trainIndex)
trainData <-  data_split$trainData
testData <- data_split$testData
# Fit multiple linear regression model
model <- lm(medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, data = trainData)
# Summarize model
summary(model)
# Make predictions on test data
predictions <- predict(model, testData)
# Evaluate model performance
rmse <- sqrt(mean((testData$medv - predictions)^2))
print(paste("Root Mean Squared Error (RMSE):", rmse))
# Visualize actual vs. predicted values
library(ggplot2)
ggplot(data.frame(actual = testData$medv, predicted = predictions), aes(x = actual, y = predicted)) +
geom_point() +
geom_abline(intercept = 0, slope = 1, color = "red") +
labs(title = "Actual vs. Predicted House Prices", x = "Actual Price", y = "Predicted Price")
labs
data_split
model <- lm(medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, data = trainData)
model
summary(model)
splitData()
lice*
5
licence(gradalgo)
licence(Gradalgo)
usethis::use_testthat(createDataPartition())
?createDataPartition
data(Boston)
data(Boston)
load("boston.rda")
write.csv(boston,"boston.csv")
write.csv(boston.rda,"boston.csv")
write.csv(Boston,"Boston.csv")
library(readr)
Boston <- read_csv("Boston.csv")
View(Boston)
Boston
load("C:/Users/LENOVO/Desktop/Gradalgo/Boston.rda")
# Load necessary libraries
library(Gradalgo)
# Load Boston Housing data
data(Boston)
Boston
# Load necessary libraries
library(Gradalgo)
# Load Boston Housing data
data(Boston)
load("C:/Users/LENOVO/Desktop/Gradalgo/Boston.rda")
# Load necessary libraries
library(Gradalgo)
# Load Boston Housing data
data(Boston)
load("C:/Users/LENOVO/Desktop/Gradalgo/Boston.rda")
Boston
# Load necessary libraries
library(Gradalgo)
# Load Boston Housing data
data(Boston)
# Load necessary libraries
library(Gradalgo)
# Load Boston Housing data
load("C:/Users/LENOVO/Desktop/Gradalgo/Boston.rda")
data(Boston)
# Load necessary libraries
library(Gradalgo)
# Load Boston Housing data
load("C:/Users/LENOVO/Desktop/Gradalgo/Boston.rda")
data(Boston)
data(Boston)
# Load necessary libraries
library(Gradalgo)
# Load Boston Housing data
load("C:/Users/LENOVO/Desktop/Gradalgo/Boston.rda")
data(Boston)
# Explore data
str(Boston)#displays the compact summary of the internal structure of the boston dataset
summary(Boston)#provides more detailed summary of each var in the boston dataset
# Split data into training and testing sets: This part splits the data set into 2 parts
#-a training set(70% of the data), - a testing set (30% of the data)
set.seed(123)# this pÃ ermites that the slpit is done thesame way every time the code is run
trainIndex <- createDataPartition(Boston$medv, p = 0.7, list = FALSE)#creates partition of the data based on the medv column
#medv specifies the column to be used for partitioning
#p=0.7 proportion of the data to include the training set (70%)
data_split <- splitData(Boston, trainIndex)
trainData <-  data_split$trainData
testData <- data_split$testData
# Fit multiple linear regression model
model <- lm(medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, data = trainData)
# Summarize model
summary(model)
# Make predictions on test data
predictions <- predict(model, testData)
# Evaluate model performance
rmse <- sqrt(mean((testData$medv - predictions)^2))
print(paste("Root Mean Squared Error (RMSE):", rmse))
# Visualize actual vs. predicted values
library(ggplot2)
ggplot(data.frame(actual = testData$medv, predicted = predictions), aes(x = actual, y = predicted)) +
geom_point() +
geom_abline(intercept = 0, slope = 1, color = "red") +
labs(title = "Actual vs. Predicted House Prices", x = "Actual Price", y = "Predicted Price")
labs
data(Boston)
library(MASS)
data("Boston")
rm(list = ls())
data("boston")
load("C:/Users/LENOVO/Desktop/Gradalgo/Boston.rda")
f <- data("Boston")
f
load("C:/Users/LENOVO/Desktop/Gradalgo/Boston.rda")
View(Boston)
View(Boston)
View(Boston)
View(Boston)
View(Boston)
View(Boston)
View(Boston)
# Load necessary libraries
library(Gradalgo)
# Load Boston Housing data
data(Boston)
rm(list = ls())
data(Boston)
rm(list = ls())
data("Boston")
rm(list = ls())
Gradalgo::Boston
load("C:/Users/LENOVO/Desktop/FILES/My func/MASS/data/Boston.rda")
View(Boston)
load("C:/Users/LENOVO/Desktop/Gradalgo/Boston.rda")
rm(list = ls())
library(Gradalgo)
Boston
MASS::Boston
library(MASS)
?Boston
save(Boston,file = "Boston.RData")
library(Gradalgo)
Gradalgo::cameroon
load("C:/Users/LENOVO/Desktop/Gradalgo/data/Cameroon.RData")
data("Boston","Gradalgo")
data("Boston","MASS")
data("Boston","Boston")
force(Boston)
save(Boston,file = "Boston.R")
save(Boston,file = "Boston.RData")
load("C:/Users/LENOVO/Desktop/Gradalgo/Boston.RData")
rm(list = ls())
Boston
#' creating  data partitioning
#'
#' @param y a number
#' @param p a number
#' @param list a vector
#'
#' @return a number
#' @export
#'
#' @examples
createDataPartition<- function(y, p, list = FALSE) {
# Set seed for reproducibility
set.seed(123)
data=Boston
# Calculate sample size
n <- length(y)
sample_size <- ceiling(p * n)
# Create indices for training data
indices <- sample(n, sample_size, replace = FALSE)
# Return indices or logical vector
if (list) {
return(split(indices, cut(indices, breaks = n, include.lowest = TRUE)))
} else {
return(indices)
}
}
#' Splitting of data
#'
#' @param data a number
#' @param indices a number
#'
#' @return a number
#' @export
#'
#' @examples
splitData <- function(data, indices) {
# Split data into training and testing sets
trainData <- data[indices, ]
testData <- data[-indices, ]
data=Boston
# Return list containing training and testing data
return(list(trainData = trainData, testData = testData))
}
data("Boston",package = "Gradalgo")
data("Boston",package = "mass")
data("Boston",package = "MASS")
devtools::document()
rm(list=c("createDataPartition","splitData")
m
rm(list=c("createDataPartition","splitData"))
devtools::document()
